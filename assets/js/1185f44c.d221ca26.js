(self.webpackChunkgosc_2021=self.webpackChunkgosc_2021||[]).push([[9732],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return d},kt:function(){return p}});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),h=c(n),p=i,f=h["".concat(l,".").concat(p)]||h[p]||u[p]||a;return n?r.createElement(f,o(o({ref:t},d),{},{components:n})):r.createElement(f,o({ref:t},d))}));function p(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<a;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},3923:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return h}});var r=n(2122),i=n(9756),a=(n(7294),n(3905)),o=["components"],s={sidebar_position:1,title:"Introduction",slug:"/2021/minerva/"},l=void 0,c={unversionedId:"2021/minerva/index",id:"2021/minerva/index",isDocsHomePage:!1,title:"Introduction",description:"\x3c!--",source:"@site/docs/2021/minerva/index.md",sourceDirName:"2021/minerva",slug:"/2021/minerva/",permalink:"/gsoc/docs/2021/minerva/",editUrl:"https://github.com/fossology/gsoc-2021/edit/master/docs/docs/2021/minerva/index.md",version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Introduction",slug:"/2021/minerva/"},sidebar:"2021",previous:{title:"Setup",permalink:"/gsoc/docs/2021/microservice/setup"},next:{title:"Week 0",permalink:"/gsoc/docs/2021/minerva/updates/2021-06-07"}},d=[{value:"A brief introduction about the project, what is the aim and what is the proposed plan.",id:"a-brief-introduction-about-the-project-what-is-the-aim-and-what-is-the-proposed-plan",children:[]},{value:"What is that I am doing right now?",id:"what-is-that-i-am-doing-right-now",children:[]}],u={toc:d};function h(e){var t=e.components,n=(0,i.Z)(e,o);return(0,a.kt)("wrapper",(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"a-brief-introduction-about-the-project-what-is-the-aim-and-what-is-the-proposed-plan"},"A brief introduction about the project, what is the aim and what is the proposed plan."),(0,a.kt)("p",null,"To implement any Machine learning/Deep learning algorithm we need a better and bigger dataset of SPDX Licences. Due to the lack of dataset currently, all the 10 algorithms which have been tested on Atarashi are restricted to 59% accuracy. There exists no such dataset for open source licenses that could be added to the existing dataset. "),(0,a.kt)("p",null,"Why do we need to create OSS Dataset - "),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"IRREGULARITY IN THE SIZE OF LICENSE TEXTS - The license texts are of different sizes with huge differences in terms of keywords count. Longer license texts contain most of the unique keywords when compared against the uniqueness of keywords in the smaller license texts. Dynamic Time Warping has the capability to tackle a varying length of array of word-embeddings of different license files.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"LICENSE TEXTS ARE DIFFERENT THAN TRADITIONAL TEXT CORPORA - Usually, in the traditional corpus, the documents are different to some extent that differentiate them with each other whereas, in license texts, most of the tokens are similar in the majority of the texts. There is a very slight difference in the use of these tokens to create a license statement. The keywords used in these license texts can be found in almost all of them with a slight variation since they all are open-source licenses stating open source softwares and underlying permissions. These similarities in licenses make them tough to be differentiated by any traditional information retrieval algorithm."))),(0,a.kt)("p",null,"So, the idea is to generate SPDX OSS license dataset using FOSSOLOGY NOMOS AGENT STRINGS.in REGEX and other SPDX released Licenses regex using python libraries and NLP algorithms. Now, to proceed with text generation using regex - we don\u2019t need to generate texts from scratch, instead, we can use an existing file as a baseline model for further manipulating and generating texts from those files. Also, we have to specify the length for text generated, because we don't want it to run till infinity. So a seed length will be taken depending on the length of each dataset available in the dataset. Further the idea extends to validation of the generated license files. "),(0,a.kt)("h2",{id:"what-is-that-i-am-doing-right-now"},"What is that I am doing right now?"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/50830709/125211400-b2ba8380-e2c3-11eb-9d44-1bab34547f04.JPG",alt:"project-overview"})),(0,a.kt)("p",null,'Till now, I have been able to fully automate the scripts to generate licenses using NLP algorithms and got them validated using Nomos. I have used "intxeger" for regex to text conversion and markov and n-gram algorithms for regex expansion. Used Nomos as a baseline to validate the generated texts. The script to generate the licenses using multiprocessing is ready and updated and I was able to generate 400000 files. I will be including text-data-augmentation using Augly on the validated files.\nI am currently working on improving the existing algorithms that I had implemented to figure out as to why a certain percentage of generated license files have been labeled as "unclassified_licenses" instead of "no_license_found" or "license_headers".'))}h.isMDXComponent=!0}}]);